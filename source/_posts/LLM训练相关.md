---
title: LLM训练相关
date: 2025-03-31 13:55:23
category: LLM基础
---

## 大模型显存占用（以 1B 参数为例）

$1B=1000^3$ 参数量
$1GB=1024^3$ Bytes

### 全参数训练（混合精度）

1. 模型参数(FP16)：2GB
2. 梯度(FP16)：2GB
3. 优化器状态（Adam 优化器 FP32）：$2*4GB+4GB=12GB$
   (因为 FP16 精度不够，优化器动量+模型参数)
4. 激活值(FP16)：$s*b*h*(34+5*a*s/h)*L/1024/1024/1024$
   **_s 是序列长度，b 是 batchsize，h 是隐藏层大小，a 是 attention 头个数，L 是层数_**

### LoRA 微调（混合精度）

1. 模型参数：2GB+LoRA 参数（$2*0.1GB$）
2. 梯度：$2*0.1GB$
3. 优化器状态（Adam 优化器 FP32）：$12*0.1GB$
4. 激活值：

### gradient checkpoints
**是的！** 计算机在进行数值运算时，**中间计算结果** **不一定** 会占用显存，具体情况取决于以下几个因素：  

1. **计算结果是否需要被存储** 🔥  
   - **不存储**：如果计算结果只是**临时值**，那么它会被存放在**寄存器（registers）** 或 **缓存（cache）** 里，不会进入显存。  
   - **需要存储**：如果计算结果稍后还要被使用（比如用于反向传播的激活值），就必须存到显存（VRAM）。  

2. **计算是在 CPU 还是 GPU 上进行** 🖥️🎮  
   - **CPU 计算**：临时值通常存在 CPU 的**寄存器** 或 **RAM**，只有必要时才写回到内存。  
   - **GPU 计算**：数值运算通常在**寄存器** 或 **共享内存** 中进行，**只有需要长期存储的值才会进入显存（VRAM）**。  

3. **是否启用了显存优化策略（如梯度检查点）** 🚀  
   - 默认情况下，**所有中间激活值都会存到显存**，以便反向传播计算梯度。  
   - 如果启用 **梯度检查点（Gradient Checkpointing）**，前向传播的激活值**不会存储**，显存占用会减少，但反向传播时要重新计算一次。

---

### **💡 举个例子：**
假设我们执行如下计算：
```python
import torch
x = torch.tensor([3.0], requires_grad=True)
y = x * 2  # y = 6
z = y ** 2  # z = 36
```
**情况 1️⃣：普通计算（不会占用显存）**
- `x * 2` 的结果 `y=6` **可能仅存于寄存器**（临时计算结果）。
- `y ** 2` 的计算完成后，`y=6` **不再被需要**，它的值会被丢弃。

**情况 2️⃣：需要反向传播（占用显存）**
```python
z.backward()  # 计算 dL/dx
```
- 由于 `z` 依赖于 `y`，而 `y` 依赖于 `x`，**PyTorch 需要存储计算图（computation graph）**，这些中间值（如 `y=6`）就会被**存到显存**。

---

### **总结**
✅ **纯计算（如 `a + b`）的中间结果**通常不会直接占用显存，因为它们只在寄存器/缓存中短暂存在。  
✅ **只有当计算结果需要被存储（如激活值用于反向传播）时，才会占用显存！**  
✅ **显存优化（如梯度检查点）可以减少存储的中间值，但计算量会增加**。  

如果你的显存不够，可以考虑 **梯度检查点（gradient_checkpointing_enable()）** 或者 **减少 batch size**！ 🚀